{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYRIT HR System Harness â€“ Scenario Summary Notebook\n",
    "\n",
    "This notebook runs a set of high-risk HR system scenarios against the PYRIT-based HR System Harness.\\n",
    "\\n",
    "The harness exercises a full **HR system** (the external FastAPI HR Simulator), not just a raw LLM.\\n",
    "It orchestrates end-to-end scenarios, calls the simulator over HTTP, scores outcomes, and writes evidence.\\n",
    "\\n",
    "In this notebook you will:\\n",
    "- Run all five HR scenarios in *simulator* mode.\\n",
    "- View a concise PASS/FAIL summary with key metrics and violation flags.\\n",
    "- Drill into a single scenario to inspect computed metrics, flags, and raw simulator outputs.\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Before running the cells below, make sure that:\\n",
    "\\n",
    "1. The external FastAPI HR Simulator is running (e.g. on `http://127.0.0.1:8600`).\\n",
    "2. The harness configuration (e.g. `pyrit_settings.local.yaml`) points `hr_simulator.base_url` at that server.\\n",
    "3. You are running this notebook from the repository root, so that the `pyrit_demo_harness` package is importable.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Ensure repository root is on sys.path so that `pyrit_demo_harness` imports cleanly.\n",
    "REPO_ROOT = Path.cwd()\n",
    "if (REPO_ROOT / 'pyrit_demo_harness').exists() and str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from pyrit_demo_harness.pyrit_integration.pyrit_runner import run_with_pyrit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scenario configuration\n",
    "\n",
    "We configure the five Annex III-4 HR scenarios exercised by this harness.\\n",
    "Each scenario has:\\n",
    "- A `scenario_id` matching the scenario JSON in `pyrit_demo_harness/scenarios/`.\\n",
    "- A short human-readable description.\\n",
    "- A key metric name used as the primary scalar indicator in the summary table.\\n",
    "\\n",
    "PASS/FAIL is determined by the harness scoring functions based on violation flags (e.g. NEG_BIAS_001).\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO_CONFIG = {\n",
    "    'HR-02-SCEN-015': {\n",
    "        'short_name': 'Self-delegation / amplification in automated screening',\n",
    "        'key_metric_name': 'self_amplification_factor',\n",
    "    },\n",
    "    'HR-02-SCEN-017': {\n",
    "        'short_name': 'Emotion / psychometric misclassification risk',\n",
    "        'key_metric_name': 'false_positive_rate',\n",
    "    },\n",
    "    'HR-02-SCEN-019': {\n",
    "        'short_name': 'Demographic leakage in profiling or scoring',\n",
    "        'key_metric_name': 'score_gap',\n",
    "    },\n",
    "    'HR-02-SCEN-022': {\n",
    "        'short_name': 'Negotiation / offer bias between majority and minority',\n",
    "        'key_metric_name': 'score_gap',\n",
    "    },\n",
    "    'HR-02-SCEN-025': {\n",
    "        'short_name': 'Performance intervention and coaching bias',\n",
    "        'key_metric_name': 'score_gap',\n",
    "    },\n",
    "}\n",
    "\n",
    "SCENARIO_IDS = list(SCENARIO_CONFIG.keys())\n",
    "\n",
    "def run_all_scenarios(mode: str = 'simulator'):\n",
    "    \"\"\"Run all configured scenarios through the HR simulator harness.\"\"\"\n",
    "    records = []\n",
    "    for scenario_id in SCENARIO_IDS:\n",
    "        print(f'Running {scenario_id} in {mode} mode...')\n",
    "        record = run_with_pyrit(scenario_id, mode=mode)\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def summarize_records(records):\n",
    "    \"\"\"Convert raw harness records into a compact pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    for record in records:\n",
    "        scenario_id = record.get('scenario_id')\n",
    "        cfg = SCENARIO_CONFIG.get(scenario_id, {})\n",
    "        key_metric_name = cfg.get('key_metric_name')\n",
    "        metrics = record.get('computed_metrics', {}) or {}\n",
    "        violation_flags = record.get('violation_flags', {}) or {}\n",
    "\n",
    "        key_metric_value = metrics.get(key_metric_name) if key_metric_name else None\n",
    "        triggered_flags = [name for name, value in violation_flags.items() if value]\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                'scenario_id': scenario_id,\n",
    "                'short_name': cfg.get('short_name', ''),\n",
    "                'pass_fail': record.get('test_result', {}).get('pass_fail'),\n",
    "                'key_metric_name': key_metric_name,\n",
    "                'key_metric_value': key_metric_value,\n",
    "                'key_violation_flags': ', '.join(triggered_flags),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty and 'scenario_id' in df.columns:\n",
    "        df = df.sort_values('scenario_id').reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run all scenarios and view summary\n",
    "\n",
    "The cell below:\\n",
    "- Executes all five HR scenarios in **simulator** mode (calling the external HR system).\\n",
    "- Collects the harness records for each scenario.\\n",
    "- Builds a pandas DataFrame summarizing PASS/FAIL, a key metric, and triggered violation flags.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all five HR scenarios against the external FastAPI HR simulator.\n",
    "# Make sure the simulator is running and `hr_simulator.base_url` is configured.\n",
    "\n",
    "records = run_all_scenarios(mode='simulator')\n",
    "\n",
    "summary_df = summarize_records(records)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the summary table\n",
    "\n",
    "- **scenario_id**: Identifier of the HR scenario (matches JSON in `pyrit_demo_harness/scenarios/`).\\n",
    "- **short_name**: Human-readable description of the scenario.\\n",
    "- **pass_fail**: Overall outcome from the harness scoring (FAIL if any violation flag is true).\\n",
    "- **key_metric_name / key_metric_value**: Primary scalar metric used as a quick indicator (e.g. `score_gap`, `false_positive_rate`, `self_amplification_factor`).\\n",
    "- **key_violation_flags**: Comma-separated list of violation flags that were triggered in this run.\\n",
    "\\n",
    "Note: PASS/FAIL is computed from the underlying **HR system** behaviour (via the simulator), not from a standalone LLM prompt.\\n",
    "The harness is exercising end-to-end system behaviour including scoring logic and any embedded models.\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect a single scenario in detail\n",
    "\n",
    "Use the cells below to drill down into one scenario: the full harness record, computed metrics, violation flags, and raw simulator outputs used for scoring.\\n",
    "You can edit `selected_scenario_id` to switch scenarios.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a scenario to inspect in detail.\n",
    "# You can change this to any of: 'HR-02-SCEN-015', 'HR-02-SCEN-017', 'HR-02-SCEN-019', 'HR-02-SCEN-022', 'HR-02-SCEN-025'.\n",
    "selected_scenario_id = 'HR-02-SCEN-022'\n",
    "\n",
    "selected_record = None\n",
    "for record in records:\n",
    "    if record.get('scenario_id') == selected_scenario_id:\n",
    "        selected_record = record\n",
    "        break\n",
    "\n",
    "if selected_record is None:\n",
    "    raise ValueError(\n",
    "        f'Scenario {selected_scenario_id!r} not found in records. '\n",
    "        'Make sure you have run the cell that executes all scenarios.'\n",
    "    )\n",
    "\n",
    "print(f'Selected scenario: {selected_scenario_id}')\n",
    "pprint(selected_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect computed metrics and violation flags for the selected scenario.\n",
    "\n",
    "metrics = selected_record.get('computed_metrics', {}) or {}\n",
    "violation_flags = selected_record.get('violation_flags', {}) or {}\n",
    "\n",
    "print('Computed metrics:')\n",
    "pprint(metrics)\n",
    "\n",
    "print('\nViolation flags (True means the condition was triggered):')\n",
    "pprint(violation_flags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect raw simulator outputs used to derive the scores.\n",
    "\n",
    "raw_results = selected_record.get('raw_results', {}) or {}\n",
    "\n",
    "print('Raw results top-level keys:', list(raw_results.keys()))\n",
    "\n",
    "# For some scenarios (e.g. negotiation / intervention / leakage), results include\n",
    "# separate entries for majority and minority personas.\n",
    "maj = raw_results.get('majority')\n",
    "minr = raw_results.get('minority')\n",
    "if isinstance(maj, dict) and isinstance(minr, dict):\n",
    "    print('\nMajority candidate result:')\n",
    "    pprint(maj)\n",
    "    print('\nMinority candidate result:')\n",
    "    pprint(minr)\n",
    "else:\n",
    "    # Fallback: pretty-print the entire raw_results structure.\n",
    "    print('\nFull raw_results:')\n",
    "    pprint(raw_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: show only scenarios that FAILED according to the harness scoring.\n",
    "\n",
    "if 'summary_df' not in globals():\n",
    "    raise RuntimeError('summary_df is not defined. Run the summary cell first.')\n",
    "\n",
    "fail_df = summary_df[summary_df['pass_fail'] == 'FAIL'].reset_index(drop=True)\n",
    "fail_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualize the key metric per scenario as a bar chart.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'summary_df' not in globals():\n",
    "    raise RuntimeError('summary_df is not defined. Run the summary cell first.')\n",
    "\n",
    "# Only plot rows where a numeric key_metric_value is available\n",
    "plot_df = summary_df.dropna(subset=['key_metric_value'])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(plot_df['scenario_id'], plot_df['key_metric_value'])\n",
    "plt.ylabel('Key metric value')\n",
    "plt.xlabel('Scenario ID')\n",
    "plt.title('Key metric per HR scenario')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
